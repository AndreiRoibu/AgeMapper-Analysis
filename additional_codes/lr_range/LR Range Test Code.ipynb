{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlogin -q short.qg -l gpu=1[affinity=true],gputype=rtx8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "cd /well/win/users/hsv459/agemapper\n",
    "\n",
    "module purge\n",
    "module load Python/3.7.4-GCCcore-8.3.0\n",
    "\n",
    "source /well/win/users/hsv459/python/functionmapper-skylakeA100/bin/activate\n",
    "\n",
    "# continue to use your python venv as normal\n",
    "   \n",
    "ipython\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from AgeMapper import AgeMapper\n",
    "from utils.data_utils import get_datasets_dynamically\n",
    "from utils.settings import Settings\n",
    "from utils.misc import my_KLDivLoss, create_folder, mae\n",
    "from utils.logging_functions import LogWriter\n",
    "from utils.early_stopping import EarlyStopping\n",
    "# Set the default floating point tensor type to FloatTensor\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "checkpoint_extension = 'path.tar'\n",
    "\n",
    "settings_file_name = 'AM0-55c.ini'\n",
    "settings = Settings(settings_file_name)\n",
    "data_parameters = settings['DATA']\n",
    "training_parameters = settings['TRAINING']\n",
    "network_parameters = settings['NETWORK']\n",
    "misc_parameters = settings['MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 1e-6\n",
    "max_lr = 10\n",
    "steps = 10500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dynamically(data_parameters):\n",
    "    print(\"Data is loading...\")\n",
    "    train_data, validation_data = get_datasets_dynamically(data_parameters)\n",
    "    print(\"Data has loaded!\")\n",
    "    print(\"Training dataset size is {}\".format(len(train_data)))\n",
    "    print(\"Validation dataset size is {}\".format(len(validation_data)))\n",
    "\n",
    "    return train_data, validation_data\n",
    "\n",
    "def train(data_parameters, training_parameters, network_parameters, misc_parameters):\n",
    "\n",
    "    if training_parameters['optimiser'] == 'adamW':\n",
    "        optimizer = torch.optim.AdamW\n",
    "    elif training_parameters['optimiser'] == 'adam':\n",
    "        optimizer = torch.optim.Adam\n",
    "    elif training_parameters['optimiser'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam # Default option\n",
    "\n",
    "    # ========================================================================================\n",
    "        \n",
    "    if training_parameters['optimiser'] == 'sgd':\n",
    "        optimizer_arguments={'lr': 1e-6,\n",
    "                            'momentum': training_parameters['optimizer_sgd_momentum'],\n",
    "                            'dampening': training_parameters['optimizer_sgd_dampening'],\n",
    "                            'weight_decay': training_parameters['optimizer_weigth_decay'],\n",
    "                            'nesterov': training_parameters['optimizer_sgd_nesterov']\n",
    "                            }\n",
    "    else:\n",
    "        optimizer_arguments={'lr': 1e-6,\n",
    "                            'betas': training_parameters['optimizer_beta'],\n",
    "                            'eps': training_parameters['optimizer_epsilon'],\n",
    "                            'weight_decay': training_parameters['optimizer_weigth_decay']\n",
    "                            }\n",
    "\n",
    "    # ========================================================================================\n",
    "        \n",
    "    if training_parameters['loss_function'] == 'mse':\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "    elif training_parameters['loss_function'] == 'kld':\n",
    "        loss_function = torch.nn.KLDivLoss()\n",
    "        print(\"Loss will return the KLD where the losses are averaged for each minibatch over observations as well as over dimensions!\")\n",
    "    elif training_parameters['loss_function'] == 'kld_batch':\n",
    "        loss_function = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        print(\"Loss will return the correct KL divergence where losses are averaged over batch dimension only!\")\n",
    "    elif training_parameters['loss_function'] == 'kld_batch_custom':\n",
    "        loss_function = my_KLDivLoss\n",
    "        print(\"Loss will return the CUSTOM correct KL divergence where losses are averaged over batch dimension only!\")\n",
    "    else:\n",
    "        print(\"Loss function not valid. Defaulting to KLD with batchmean reduction!\")\n",
    "        loss_function = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    train_data, validation_data = load_data_dynamically(data_parameters)\n",
    "    train_loader = data.DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=training_parameters['training_batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=data_parameters['num_workers']\n",
    "    )\n",
    "    validation_loader = data.DataLoader(\n",
    "        dataset=validation_data,\n",
    "        batch_size=training_parameters['validation_batch_size'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=data_parameters['num_workers']\n",
    "    )\n",
    "\n",
    "    if network_parameters['network_name'] == 'AgeMapper_N1':\n",
    "        from AgeMapper import AgeMapper_N1\n",
    "        AgeMapperModel = AgeMapper_N1()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N2':\n",
    "        from AgeMapper import AgeMapper_N2\n",
    "        AgeMapperModel = AgeMapper_N2()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N3':\n",
    "        from AgeMapper import AgeMapper_N3\n",
    "        AgeMapperModel = AgeMapper_N3()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N4':\n",
    "        from AgeMapper import AgeMapper_N4\n",
    "        AgeMapperModel = AgeMapper_N4()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N5':\n",
    "        from AgeMapper import AgeMapper_N5\n",
    "        AgeMapperModel = AgeMapper_N5()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N6':\n",
    "        from AgeMapper import AgeMapper_N6\n",
    "        AgeMapperModel = AgeMapper_N6()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N7':\n",
    "        from AgeMapper import AgeMapper_N7\n",
    "        AgeMapperModel = AgeMapper_N7()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N8':\n",
    "        from AgeMapper import AgeMapper_N8\n",
    "        AgeMapperModel = AgeMapper_N8()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N9':\n",
    "        from AgeMapper import AgeMapper_N9\n",
    "        AgeMapperModel = AgeMapper_N9()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N10':\n",
    "        from AgeMapper import AgeMapper_N10\n",
    "        AgeMapperModel = AgeMapper_N10()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N11':\n",
    "        from AgeMapper import AgeMapper_N11\n",
    "        AgeMapperModel = AgeMapper_N11()\n",
    "    elif network_parameters['network_name'] == 'AgeMapper_N12':\n",
    "        from AgeMapper import AgeMapper_N12\n",
    "        AgeMapperModel = AgeMapper_N12()\n",
    "    else:   \n",
    "        AgeMapperModel = AgeMapper()\n",
    "\n",
    "    solver = Solver(model=AgeMapperModel,\n",
    "                    number_of_classes=network_parameters['number_of_classes'],\n",
    "                    experiment_name=training_parameters['experiment_name'],\n",
    "                    optimizer=optimizer,\n",
    "                    optimizer_arguments=optimizer_arguments,\n",
    "                    loss_function=loss_function,\n",
    "                    model_name=training_parameters['experiment_name'],\n",
    "                    number_epochs=training_parameters['number_of_epochs'],\n",
    "                    loss_log_period=training_parameters['loss_log_period'],\n",
    "                    learning_rate_scheduler_step_size=training_parameters['learning_rate_scheduler_step_size'],\n",
    "                    learning_rate_scheduler_gamma=training_parameters['learning_rate_scheduler_gamma'],\n",
    "                    use_last_checkpoint=training_parameters['use_last_checkpoint'],\n",
    "                    experiment_directory=misc_parameters['experiments_directory'],\n",
    "                    logs_directory=misc_parameters['logs_directory'],\n",
    "                    checkpoint_directory=misc_parameters['checkpoint_directory'],\n",
    "                    best_checkpoint_directory=misc_parameters['best_checkpoint_directory'],\n",
    "                    save_model_directory=misc_parameters['save_model_directory'],\n",
    "                    learning_rate_validation_scheduler=training_parameters['learning_rate_validation_scheduler'],\n",
    "                    learning_rate_cyclical = training_parameters['learning_rate_cyclical'],\n",
    "                    learning_rate_scheduler_patience=training_parameters['learning_rate_scheduler_patience'],\n",
    "                    learning_rate_scheduler_threshold=training_parameters['learning_rate_scheduler_threshold'],\n",
    "                    learning_rate_scheduler_min_value=training_parameters['learning_rate_scheduler_min_value'],\n",
    "                    learning_rate_scheduler_max_value=training_parameters['learning_rate_scheduler_max_value'],\n",
    "                    learning_rate_scheduler_step_number=training_parameters['learning_rate_scheduler_step_number'],\n",
    "                    early_stopping_patience=training_parameters['early_stopping_patience'],\n",
    "                    early_stopping_min_delta=training_parameters['early_stopping_min_delta'],\n",
    "                    age_prediction_loss_flag=training_parameters['age_prediction_loss_flag']\n",
    "                    )\n",
    "\n",
    "    solver.train(train_loader, validation_loader)\n",
    "\n",
    "    del train_data, validation_data, train_loader, validation_loader, AgeMapperModel, solver, optimizer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 number_of_classes,\n",
    "                 experiment_name,\n",
    "                 optimizer,\n",
    "                 optimizer_arguments={},\n",
    "                 loss_function=torch.nn.MSELoss(),\n",
    "                 model_name='BrainMapper',\n",
    "                 number_epochs=10,\n",
    "                 loss_log_period=5,\n",
    "                 learning_rate_scheduler_step_size=5,\n",
    "                 learning_rate_scheduler_gamma=0.5,\n",
    "                 use_last_checkpoint=True,\n",
    "                 experiment_directory='experiments',\n",
    "                 logs_directory='logs',\n",
    "                 checkpoint_directory='checkpoints',\n",
    "                 best_checkpoint_directory = 'best_checkpoint_directory',\n",
    "                 save_model_directory='saved_models',\n",
    "                 learning_rate_validation_scheduler = False,\n",
    "                 learning_rate_cyclical = False,\n",
    "                 learning_rate_scheduler_patience=5,\n",
    "                 learning_rate_scheduler_threshold=1e-6,\n",
    "                 learning_rate_scheduler_min_value=5e-6,\n",
    "                 learning_rate_scheduler_max_value=5e-5,\n",
    "                 learning_rate_scheduler_step_number=13200,\n",
    "                 early_stopping_patience=10,\n",
    "                 early_stopping_min_delta=0,\n",
    "                 age_prediction_loss_flag=False\n",
    "                 ):\n",
    "        \n",
    "        self.age_prediction_loss_flag = age_prediction_loss_flag\n",
    "\n",
    "        self.model = model\n",
    "#         print(model)\n",
    "        self.parallelism = False\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device is \"cpu\":\n",
    "            print(\"WARNING: Default device is CPU, not GPU!\")\n",
    "        elif torch.cuda.device_count()>1:\n",
    "            self.parallelism = True\n",
    "            print(\"ATTENTION! Multiple GPUs detected. {} GPUs will be used for training\".format(torch.cuda.device_count()))\n",
    "        else:\n",
    "            print(\"A single GPU detected\")\n",
    "\n",
    "        if optimizer_arguments['weight_decay']!=0:\n",
    "            prelus = {name for name, module in model.named_modules() if isinstance(module, torch.nn.PReLU)}\n",
    "            prelu_parameter_names = {name for name, _ in model.named_parameters() if name.rsplit('.', 1)[0] in prelus}\n",
    "            parameters = [\n",
    "                {'params': [parameter for parameter_name, parameter in model.named_parameters() if parameter_name not in prelu_parameter_names]},\n",
    "                {'params': [parameter for parameter_name, parameter in model.named_parameters() if parameter_name in prelu_parameter_names], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        else:\n",
    "            parameters = model.parameters()\n",
    "        self.optimizer = optimizer(parameters, **optimizer_arguments)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            if hasattr(loss_function, 'to'):\n",
    "                self.loss_function = loss_function.to(self.device)\n",
    "            else:\n",
    "                self.loss_function = loss_function\n",
    "\n",
    "        else:\n",
    "            self.loss_function = loss_function\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.number_epochs = number_epochs\n",
    "        self.loss_log_period = loss_log_period  \n",
    "\n",
    "        self.use_last_checkpoint = use_last_checkpoint\n",
    "\n",
    "        experiment_directory_path = os.path.join(experiment_directory, experiment_name)\n",
    "        self.experiment_directory_path = experiment_directory_path\n",
    "\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.best_checkpoint_directory = best_checkpoint_directory\n",
    "\n",
    "        create_folder(experiment_directory)\n",
    "        create_folder(experiment_directory_path)\n",
    "        create_folder(os.path.join(experiment_directory_path, self.checkpoint_directory))\n",
    "        create_folder(os.path.join(experiment_directory_path, self.best_checkpoint_directory))\n",
    "\n",
    "        self.start_epoch = 1\n",
    "        self.start_iteration = 1\n",
    "\n",
    "        self.LogWriter = LogWriter(number_of_classes=number_of_classes,\n",
    "                                   logs_directory=logs_directory,\n",
    "                                   experiment_name=experiment_name,\n",
    "                                   use_last_checkpoint=use_last_checkpoint\n",
    "                                   )\n",
    "\n",
    "        self.early_stop = False\n",
    "\n",
    "        self.save_model_directory = save_model_directory\n",
    "        self.final_model_output_file = experiment_name + \".pth.tar\"\n",
    "\n",
    "        self.best_score_early_stop = None\n",
    "        self.counter_early_stop = 0\n",
    "        self.previous_loss = None\n",
    "        self.valid_epoch = None\n",
    "        self.previous_age_deltas = None\n",
    "\n",
    "        if use_last_checkpoint:\n",
    "            self.load_checkpoint()\n",
    "            self.EarlyStopping = EarlyStopping(patience=early_stopping_patience, min_delta=early_stopping_min_delta, best_score=self.best_score_early_stop, counter=self.counter_early_stop)\n",
    "        else:\n",
    "            self.EarlyStopping = EarlyStopping(patience=early_stopping_patience, min_delta=early_stopping_min_delta)\n",
    "\n",
    "        self.bin_centers = np.load(\"datasets/bin_centers.npy\")\n",
    "        \n",
    "        \n",
    "        # ========================================================================================\n",
    "        \n",
    "        # SET batch = 8, dataset = male large, AM0-25\n",
    "        \n",
    "        \n",
    "#         self.max_iterations = 10500\n",
    "\n",
    "        self.experiment_name = experiment_name\n",
    "        \n",
    "#         self.learning_rate_scheduler = lr_scheduler.CyclicLR(optimizer=self.optimizer,\n",
    "#                                                             base_lr = 1e-7,\n",
    "#                                                             max_lr = 10,\n",
    "#                                                             step_size_up=self.max_iterations,\n",
    "#                                                             cycle_momentum=False,\n",
    "#                                                             mode = 'exp_range',\n",
    "#                                                             verbose=True,\n",
    "#                                                             )\n",
    "\n",
    "#         self.learning_rate_scheduler = lr_scheduler.StepLR(optimizer=self.optimizer,\n",
    "#                                                             step_size=learning_rate_scheduler_step_size,\n",
    "#                                                             gamma=learning_rate_scheduler_gamma)\n",
    "        \n",
    "        # ========================================================================================\n",
    "        \n",
    "\n",
    "    def train(self, train_loader, validation_loader):\n",
    "\n",
    "#         model, optimizer, learning_rate_scheduler = self.model, self.optimizer, self.learning_rate_scheduler\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "\n",
    "\n",
    "        # ========================================================================================\n",
    "        \n",
    "        output_statistics = {}\n",
    "        output_statistics_name = \"lr_range_test_\" + self.experiment_name + '_4' + \".csv\"\n",
    "        create_folder(\"lr_range_tests\")\n",
    "        output_statistics_path = os.path.join(\"lr_range_tests\", output_statistics_name)\n",
    "        \n",
    "        number_of_steps = 6\n",
    "        \n",
    "        \n",
    "        lrs = []\n",
    "        steps=1000\n",
    "        self.max_iterations = number_of_steps * steps # max 7500 (30 epochs @ 3000 subs with batch=12)\n",
    "        for i in range(number_of_steps):\n",
    "            base_lr = 10**-i\n",
    "            max_lr = 10**-(i+1)\n",
    "            lr = np.linspace(base_lr, max_lr, steps)\n",
    "            if i!=(number_of_steps-1):\n",
    "                lr = lr[:-1]\n",
    "            lr = lr.tolist()\n",
    "            lrs += lr\n",
    "            \n",
    "        lrs = np.array(lrs)\n",
    "        lrs = lrs.flatten()\n",
    "        lrs = np.flip(lrs)\n",
    "#         print(lrs)\n",
    "        \n",
    "        lrs_counter = 0\n",
    "        \n",
    "        optimizer.param_groups[0]['lr'] = lrs[lrs_counter]\n",
    "#         optimizer.param_groups[1]['lr'] = lrs[lrs_counter]\n",
    "        \n",
    "        print('LEARNING RATE 0=', optimizer.param_groups[0]['lr'])\n",
    "#         print('LEARNING RATE 0=', optimizer.param_groups[1]['lr'])\n",
    "        \n",
    "        # ========================================================================================\n",
    "        \n",
    "        \n",
    "        dataloaders = {'train': train_loader}\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # clear memory\n",
    "            model.to(self.device)  # Moving the model to GPU\n",
    "\n",
    "        print('****************************************************************')\n",
    "        print('TRAINING IS STARTING!')\n",
    "        print('=====================')\n",
    "        print('Model Name: {}'.format(self.model_name))\n",
    "        if torch.cuda.is_available():\n",
    "            print('Device Type: {}'.format(\n",
    "                torch.cuda.get_device_name(self.device)))\n",
    "        else:\n",
    "            print('Device Type: {}'.format(self.device))\n",
    "        start_time = datetime.now()\n",
    "        print('Started At: {}'.format(start_time))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        iteration = self.start_iteration\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.number_epochs+1):\n",
    "\n",
    "            print(\"Epoch {}/{}\".format(epoch, self.number_epochs))\n",
    "\n",
    "            for phase in ['train']:\n",
    "                print('-> Phase: {}'.format(phase))\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_index, sampled_batch in enumerate(dataloaders[phase]):\n",
    "                    X = sampled_batch[0].type(torch.FloatTensor)\n",
    "                    y = sampled_batch[1].type(torch.FloatTensor)\n",
    "                    y += 1e-16 # to prevent log(0) problem\n",
    "                    y_age = sampled_batch[2]\n",
    "\n",
    "                    # We add an extra dimension (~ number of channels) for the 3D convolutions.\n",
    "                    if len(X.size())<5:\n",
    "                        X = torch.unsqueeze(X, dim=1)\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        X = X.cuda(self.device, non_blocking=True)\n",
    "                        y = y.cuda(self.device, non_blocking=True)\n",
    "\n",
    "                    y_hat = model(X)   # Forward pass\n",
    "                    \n",
    "                    if self.age_prediction_loss_flag == True:\n",
    "                        if torch.cuda.is_available():\n",
    "                            y_age = y_age.type(torch.FloatTensor)\n",
    "                            y_age = y_age.reshape(-1,1)\n",
    "                            y_age = y_age.cuda(self.device, non_blocking=True)\n",
    "                            \n",
    "#                         print(y_hat.shape, y_age.shape)\n",
    "                        loss = self.loss_function(y_hat, y_age)\n",
    "\n",
    "                        y_hat_age = np.float32(y_hat.detach().cpu().numpy())\n",
    "                        y_age = np.float32(y_age.detach().cpu().numpy())\n",
    "                        age_delta = mae(y_hat_age, y_age)\n",
    "                    else:\n",
    "                        y_hat = torch.squeeze(y_hat)\n",
    "                        loss = self.loss_function(y_hat, y) \n",
    "                        y_hat = np.float32(y_hat.detach().cpu().numpy())\n",
    "                        y_hat_age = np.matmul(np.exp(y_hat), self.bin_centers)\n",
    "                        y_age = np.float32(y_age.detach().cpu().numpy())\n",
    "                        age_delta = mae(y_hat_age, y_age)\n",
    "\n",
    "\n",
    "                    optimizer.zero_grad()  # Zero the parameter gradients\n",
    "                    loss.backward()  # Backward propagation\n",
    "                    optimizer.step()\n",
    "\n",
    "# ========================================================================================\n",
    "\n",
    "                    if lrs_counter<(len(lrs)-1):\n",
    "                        lrs_counter+=1\n",
    "\n",
    "                        optimizer.param_groups[0]['lr'] = lrs[lrs_counter]\n",
    "    #                     optimizer.param_groups[1]['lr'] = lrs[lrs_counter]\n",
    "\n",
    "                        print('----> LEARNING RATE = ', optimizer.param_groups[0]['lr'])\n",
    "    #                     print('----> LEARNING RATE = ', optimizer.param_groups[1]['lr'])\n",
    "        \n",
    "# ========================================================================================\n",
    "\n",
    "                    self.LogWriter.loss_per_iteration(loss.item(), batch_index, iteration)\n",
    "                    self.LogWriter.learning_rate_per_iteration(optimizer.param_groups[0]['lr'], batch_index, iteration)\n",
    "\n",
    "                    output_statistics[iteration] = [iteration, loss.item(), age_delta, optimizer.param_groups[0]['lr']]\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # Clear the memory\n",
    "\n",
    "                    del X, y, y_hat, loss, y_hat_age, y_age\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "#                     learning_rate_scheduler.step()\n",
    "\n",
    "                    if iteration == self.max_iterations:\n",
    "                        break\n",
    "\n",
    "                if iteration == self.max_iterations:\n",
    "                    break\n",
    "\n",
    "            if iteration == self.max_iterations:\n",
    "                break\n",
    "\n",
    "            print(\"Epoch {}/{} DONE!\".format(epoch, self.number_epochs))\n",
    "            \n",
    "        output_statistics_df = pd.DataFrame.from_dict(output_statistics, orient='index', columns=['iteration', 'kldloss', 'agedelta', 'lr'])     \n",
    "        output_statistics_df.to_csv(output_statistics_path)\n",
    "\n",
    "        self.LogWriter.close()\n",
    "\n",
    "        print('----------------------------------------')\n",
    "        print('NO TRAINING DONE TO PREVENT OVERFITTING!')\n",
    "        print('=====================')\n",
    "        end_time = datetime.now()\n",
    "        print('Completed At: {}'.format(end_time))\n",
    "        print('Training Duration: {}'.format(end_time - start_time))\n",
    "        print('****************************************************************')\n",
    "        \n",
    "train(data_parameters, training_parameters, network_parameters, misc_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(data_parameters, training_parameters, network_parameters, misc_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
