{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlogin -q short.qg -l gpu=1[affinity=true],gputype=rtx8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "cd /well/win/users/hsv459/agemapper\n",
    "\n",
    "module purge\n",
    "module load Python/3.7.4-GCCcore-8.3.0\n",
    "\n",
    "source /well/win/users/hsv459/python/functionmapper-skylakeA100/bin/activate\n",
    "\n",
    "# continue to use your python venv as normal\n",
    "   \n",
    "ipython\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn import L1Loss\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from utils.misc import create_folder, mae\n",
    "from utils.logging_functions import LogWriter\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.data_utils import get_datasets, get_datasets_dynamically\n",
    "from AgeMapper import AgeMapper\n",
    "from utils.settings import Settings\n",
    "\n",
    "checkpoint_extension = 'path.tar'\n",
    "\n",
    "settings_file_name = 'AM0-399.ini'\n",
    "settings = Settings(settings_file_name)\n",
    "data_parameters = settings['DATA']\n",
    "training_parameters = settings['TRAINING']\n",
    "network_parameters = settings['NETWORK']\n",
    "misc_parameters = settings['MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 1e-4\n",
    "max_lr = 0.5\n",
    "steps = 10500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dynamically(data_parameters):\n",
    "    print(\"Data is loading...\")\n",
    "    train_data, validation_data, resolution = get_datasets_dynamically(data_parameters)\n",
    "    print(\"Data has loaded!\")\n",
    "    print(\"Training dataset size is {}\".format(len(train_data)))\n",
    "    print(\"Validation dataset size is {}\".format(len(validation_data)))\n",
    "\n",
    "    return train_data, validation_data, resolution\n",
    "\n",
    "def train(data_parameters, training_parameters, network_parameters, misc_parameters,\n",
    "         base_lr):\n",
    "\n",
    "    if training_parameters['optimiser'] == 'adamW':\n",
    "        optimizer = torch.optim.AdamW\n",
    "    elif training_parameters['optimiser'] == 'adam':\n",
    "        optimizer = torch.optim.Adam\n",
    "    elif training_parameters['optimiser'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam # Default option\n",
    "\n",
    "    # ========================================================================================\n",
    "        \n",
    "\n",
    "    optimizer_arguments={'lr': base_lr,\n",
    "                        'betas': training_parameters['optimizer_beta'],\n",
    "                        'eps': training_parameters['optimizer_epsilon'],\n",
    "                        'weight_decay': training_parameters['optimizer_weigth_decay']\n",
    "                        }\n",
    "\n",
    "    # ========================================================================================\n",
    "        \n",
    "    if training_parameters['loss_function'] == 'mse':\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "    elif training_parameters['loss_function'] == 'mae':\n",
    "        loss_function = torch.nn.L1Loss()\n",
    "    else:\n",
    "        print(\"Loss function not valid. Defaulting to MSE!\")\n",
    "        loss_function = torch.nn.MSELoss(reduction='batchmean')\n",
    "\n",
    "    train_data, validation_data, resolution = load_data_dynamically(data_parameters)\n",
    "    train_loader = data.DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=training_parameters['training_batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=data_parameters['num_workers']\n",
    "    )\n",
    "    validation_loader = data.DataLoader(\n",
    "        dataset=validation_data,\n",
    "        batch_size=training_parameters['validation_batch_size'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=data_parameters['num_workers']\n",
    "    )\n",
    "\n",
    "    # dropout_rate_2 = network_parameters['dropout_rate_2']\n",
    "    # dropout_rate_3 = network_parameters['dropout_rate_3']\n",
    "    \n",
    "    if training_parameters['use_pre_trained']:\n",
    "        pre_trained_path = \"saved_models/\" + training_parameters['experiment_name'] + \".pth.tar\"\n",
    "        AgeMapperModel = torch.load(pre_trained_path)\n",
    "    else:\n",
    "        AgeMapperModel = AgeMapper(resolution=resolution,\n",
    "                                    # dropout_rate_2=dropout_rate_2,\n",
    "                                    # dropout_rate_3=dropout_rate_3,\n",
    "                                    )\n",
    "\n",
    "    solver = Solver(model=AgeMapperModel,\n",
    "                    number_of_classes=network_parameters['number_of_classes'],\n",
    "                    experiment_name=training_parameters['experiment_name'],\n",
    "                    optimizer=optimizer,\n",
    "                    optimizer_arguments=optimizer_arguments,\n",
    "                    loss_function=loss_function,\n",
    "                    model_name=training_parameters['experiment_name'],\n",
    "                    number_epochs=training_parameters['number_of_epochs'],\n",
    "                    loss_log_period=training_parameters['loss_log_period'],\n",
    "                    learning_rate_scheduler_step_size=training_parameters['learning_rate_scheduler_step_size'],\n",
    "                    learning_rate_scheduler_gamma=training_parameters['learning_rate_scheduler_gamma'],\n",
    "                    use_last_checkpoint=training_parameters['use_last_checkpoint'],\n",
    "                    experiment_directory=misc_parameters['experiments_directory'],\n",
    "                    logs_directory=misc_parameters['logs_directory'],\n",
    "                    checkpoint_directory=misc_parameters['checkpoint_directory'],\n",
    "                    best_checkpoint_directory=misc_parameters['best_checkpoint_directory'],\n",
    "                    save_model_directory=misc_parameters['save_model_directory'],\n",
    "                    learning_rate_validation_scheduler=training_parameters['learning_rate_validation_scheduler'],\n",
    "                    learning_rate_cyclical = training_parameters['learning_rate_cyclical'],\n",
    "                    learning_rate_scheduler_patience=training_parameters['learning_rate_scheduler_patience'],\n",
    "                    learning_rate_scheduler_threshold=training_parameters['learning_rate_scheduler_threshold'],\n",
    "                    learning_rate_scheduler_min_value=training_parameters['learning_rate_scheduler_min_value'],\n",
    "                    learning_rate_scheduler_max_value=training_parameters['learning_rate_scheduler_max_value'],\n",
    "                    learning_rate_scheduler_step_number=training_parameters['learning_rate_scheduler_step_number'],\n",
    "                    early_stopping_patience=training_parameters['early_stopping_patience'],\n",
    "                    early_stopping_min_delta=training_parameters['early_stopping_min_delta'],\n",
    "                    )\n",
    "\n",
    "    solver.train(train_loader, validation_loader)\n",
    "\n",
    "    del train_data, validation_data, train_loader, validation_loader, AgeMapperModel, solver, optimizer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 number_of_classes,\n",
    "                 experiment_name,\n",
    "                 optimizer,\n",
    "                 optimizer_arguments={},\n",
    "                 loss_function=torch.nn.MSELoss(),\n",
    "                 model_name='BrainMapper',\n",
    "                 number_epochs=10,\n",
    "                 loss_log_period=5,\n",
    "                 learning_rate_scheduler_step_size=5,\n",
    "                 learning_rate_scheduler_gamma=0.5,\n",
    "                 use_last_checkpoint=True,\n",
    "                 experiment_directory='experiments',\n",
    "                 logs_directory='logs',\n",
    "                 checkpoint_directory='checkpoints',\n",
    "                 best_checkpoint_directory = 'best_checkpoint_directory',\n",
    "                 save_model_directory='saved_models',\n",
    "                 learning_rate_validation_scheduler = False,\n",
    "                 learning_rate_cyclical = False,\n",
    "                 learning_rate_scheduler_patience=5,\n",
    "                 learning_rate_scheduler_threshold=1e-6,\n",
    "                 learning_rate_scheduler_min_value=5e-6,\n",
    "                 learning_rate_scheduler_max_value=5e-5,\n",
    "                 learning_rate_scheduler_step_number=13200,\n",
    "                 early_stopping_patience=10,\n",
    "                 early_stopping_min_delta=0,\n",
    "                 ):\n",
    "        \n",
    "\n",
    "        self.model = model\n",
    "#         print(model)\n",
    "        self.parallelism = False\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device == \"cpu\":\n",
    "            print(\"WARNING: Default device is CPU, not GPU!\")\n",
    "        elif torch.cuda.device_count()>1:\n",
    "            self.parallelism = True\n",
    "            print(\"ATTENTION! Multiple GPUs detected. {} GPUs will be used for training\".format(torch.cuda.device_count()))\n",
    "        else:\n",
    "            print(\"A single GPU detected\")\n",
    "\n",
    "        if optimizer_arguments['weight_decay']!=0:\n",
    "            prelus = {name for name, module in model.named_modules() if isinstance(module, torch.nn.PReLU)}\n",
    "            prelu_parameter_names = {name for name, _ in model.named_parameters() if name.rsplit('.', 1)[0] in prelus}\n",
    "            parameters = [\n",
    "                {'params': [parameter for parameter_name, parameter in model.named_parameters() if parameter_name not in prelu_parameter_names]},\n",
    "                {'params': [parameter for parameter_name, parameter in model.named_parameters() if parameter_name in prelu_parameter_names], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        else:\n",
    "            parameters = model.parameters()\n",
    "        self.optimizer = optimizer(parameters, **optimizer_arguments)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            if hasattr(loss_function, 'to'):\n",
    "                self.loss_function = loss_function.to(self.device)\n",
    "                self.MAE = L1Loss().to(self.device)\n",
    "            else:\n",
    "                self.loss_function = loss_function\n",
    "                self.MAE = L1Loss()\n",
    "\n",
    "        else:\n",
    "            self.loss_function = loss_function\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.number_epochs = number_epochs\n",
    "        self.loss_log_period = loss_log_period  \n",
    "\n",
    "        self.use_last_checkpoint = use_last_checkpoint\n",
    "\n",
    "        experiment_directory_path = os.path.join(experiment_directory, experiment_name)\n",
    "        self.experiment_directory_path = experiment_directory_path\n",
    "\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.best_checkpoint_directory = best_checkpoint_directory\n",
    "\n",
    "        create_folder(experiment_directory)\n",
    "        create_folder(experiment_directory_path)\n",
    "        create_folder(os.path.join(experiment_directory_path, self.checkpoint_directory))\n",
    "        create_folder(os.path.join(experiment_directory_path, self.best_checkpoint_directory))\n",
    "\n",
    "        self.start_epoch = 1\n",
    "        self.start_iteration = 1\n",
    "\n",
    "        self.LogWriter = LogWriter(number_of_classes=number_of_classes,\n",
    "                                   logs_directory=logs_directory,\n",
    "                                   experiment_name=experiment_name,\n",
    "                                   use_last_checkpoint=use_last_checkpoint\n",
    "                                   )\n",
    "\n",
    "        self.early_stop = False\n",
    "\n",
    "        self.save_model_directory = save_model_directory\n",
    "        self.final_model_output_file = experiment_name + \".pth.tar\"\n",
    "\n",
    "        self.best_score_early_stop = None\n",
    "        self.counter_early_stop = 0\n",
    "        self.previous_loss = None\n",
    "        self.valid_epoch = None\n",
    "        self.previous_age_deltas = None\n",
    "\n",
    "        if use_last_checkpoint:\n",
    "            self.load_checkpoint()\n",
    "            self.EarlyStopping = EarlyStopping(patience=early_stopping_patience, min_delta=early_stopping_min_delta, best_score=self.best_score_early_stop, counter=self.counter_early_stop)\n",
    "        else:\n",
    "            self.EarlyStopping = EarlyStopping(patience=early_stopping_patience, min_delta=early_stopping_min_delta)\n",
    "\n",
    "        self.bin_centers = np.load(\"datasets/bin_centers.npy\")\n",
    "        \n",
    "        \n",
    "        # ========================================================================================\n",
    "\n",
    "        \n",
    "        self.experiment_name = experiment_name\n",
    "        \n",
    "        \n",
    "        #### LINEAR / EXPONENTIAL CASES\n",
    "#         SET batch = 8, dataset = male large, AM0-25\n",
    "        \n",
    "        \n",
    "        self.max_iterations = 1000 # 250 iterations / epoch\n",
    "        self.learning_rate_scheduler = lr_scheduler.CyclicLR(optimizer=self.optimizer,\n",
    "                                                            base_lr = 1e-4,\n",
    "                                                            max_lr = 1e-1,\n",
    "                                                            step_size_up=self.max_iterations,\n",
    "                                                            cycle_momentum=False,\n",
    "                                                            mode = 'triangular',\n",
    "                                                            verbose=True,\n",
    "                                                            )\n",
    "\n",
    "#         self.learning_rate_scheduler = lr_scheduler.StepLR(optimizer=self.optimizer,\n",
    "#                                                             step_size=learning_rate_scheduler_step_size,\n",
    "#                                                             gamma=learning_rate_scheduler_gamma)\n",
    "        \n",
    "        # ========================================================================================\n",
    "        \n",
    "\n",
    "    def train(self, train_loader, validation_loader):\n",
    "        \n",
    "        output_statistics = {}\n",
    "        output_statistics_name = \"lr_range_test_\" + self.experiment_name + '_2' + \".csv\"\n",
    "        create_folder(\"lr_range_tests\")\n",
    "        output_statistics_path = os.path.join(\"lr_range_tests\", output_statistics_name)\n",
    "        \n",
    "        #### LINEAR / EXPONENTIAL CASES\n",
    "        model, optimizer, learning_rate_scheduler = self.model, self.optimizer, self.learning_rate_scheduler\n",
    "    \n",
    "        #### LINEAR UNIFORM CASE\n",
    "#         model, optimizer = self.model, self.optimizer\n",
    "\n",
    "\n",
    "        # ========================================================================================\n",
    "        #### LINEAR UNIFORM CASE\n",
    "        \n",
    "        \n",
    "#         number_of_steps = 5\n",
    "        \n",
    "        \n",
    "#         lrs = []\n",
    "#         steps=100\n",
    "#         self.max_iterations = number_of_steps * steps # max 7500 (30 epochs @ 3000 subs with batch=12 = 250/e)\n",
    "#         for i in range(number_of_steps):\n",
    "#             base_lr = 10**-i\n",
    "#             max_lr = 10**-(i+1)\n",
    "#             lr = np.linspace(base_lr, max_lr, steps)\n",
    "#             if i!=(number_of_steps-1):\n",
    "#                 lr = lr[:-1]\n",
    "#             lr = lr.tolist()\n",
    "#             lrs += lr\n",
    "            \n",
    "#         lrs = np.array(lrs)\n",
    "#         lrs = lrs.flatten()\n",
    "#         lrs = np.flip(lrs)\n",
    "# #         print(lrs)\n",
    "        \n",
    "#         lrs_counter = 0\n",
    "        \n",
    "#         optimizer.param_groups[0]['lr'] = lrs[lrs_counter]\n",
    "# #         optimizer.param_groups[1]['lr'] = lrs[lrs_counter]\n",
    "        \n",
    "\n",
    "        \n",
    "        # ========================================================================================\n",
    "\n",
    "        print('LEARNING RATE 0=', optimizer.param_groups[0]['lr'])\n",
    "#         print('LEARNING RATE 0=', optimizer.param_groups[1]['lr'])\n",
    "        \n",
    "        dataloaders = {'train': train_loader}\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # clear memory\n",
    "            model.to(self.device)  # Moving the model to GPU\n",
    "\n",
    "        print('****************************************************************')\n",
    "        print('TRAINING IS STARTING!')\n",
    "        print('=====================')\n",
    "        print('Model Name: {}'.format(self.model_name))\n",
    "        if torch.cuda.is_available():\n",
    "            print('Device Type: {}'.format(\n",
    "                torch.cuda.get_device_name(self.device)))\n",
    "        else:\n",
    "            print('Device Type: {}'.format(self.device))\n",
    "        start_time = datetime.now()\n",
    "        print('Started At: {}'.format(start_time))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        iteration = self.start_iteration\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.number_epochs+1):\n",
    "\n",
    "            print(\"Epoch {}/{}\".format(epoch, self.number_epochs))\n",
    "\n",
    "            for phase in ['train']:\n",
    "                print('-> Phase: {}'.format(phase))\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_index, sampled_batch in enumerate(dataloaders[phase]):\n",
    "                    X = sampled_batch[0].type(torch.FloatTensor)\n",
    "                    y_age = sampled_batch[1].type(torch.FloatTensor)\n",
    "                    y_age = y_age.reshape(-1,1)\n",
    "\n",
    "                    # We add an extra dimension (~ number of channels) for the 3D convolutions.\n",
    "                    if len(X.size())<5:\n",
    "                        X = torch.unsqueeze(X, dim=1)\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        X = X.cuda(self.device, non_blocking=True)\n",
    "                        y_age = y_age.cuda(self.device, non_blocking=True)\n",
    "\n",
    "                    y_hat = model(X)   # Forward pass\n",
    "                    loss = self.loss_function(y_hat, y_age)\n",
    "                    age_delta = self.MAE(y_hat, y_age)\n",
    "                    age_delta = age_delta.item()\n",
    "                    \n",
    "\n",
    "                    optimizer.zero_grad()  # Zero the parameter gradients\n",
    "                    loss.backward()  # Backward propagation\n",
    "                    optimizer.step()\n",
    "\n",
    "# ========================================================================================\n",
    "                    #### LINEAR UNIFORM CASE\n",
    "    \n",
    "#                     if lrs_counter<(len(lrs)-1):\n",
    "#                         lrs_counter+=1\n",
    "\n",
    "#                         optimizer.param_groups[0]['lr'] = lrs[lrs_counter]\n",
    "#     #                     optimizer.param_groups[1]['lr'] = lrs[lrs_counter]\n",
    "\n",
    "#                         print('----> LEARNING RATE = ', optimizer.param_groups[0]['lr'])\n",
    "#     #                     print('----> LEARNING RATE = ', optimizer.param_groups[1]['lr'])\n",
    "        \n",
    "# ========================================================================================\n",
    "\n",
    "                    self.LogWriter.loss_per_iteration(loss.item(), batch_index, iteration)\n",
    "                    self.LogWriter.learning_rate_per_iteration(optimizer.param_groups[0]['lr'], batch_index, iteration)\n",
    "\n",
    "                    output_statistics[iteration] = [iteration, loss.item(), age_delta, \n",
    "                                                    optimizer.param_groups[0]['lr']]\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # Clear the memory\n",
    "\n",
    "                    del X, y_hat, loss, y_age, age_delta\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    #### LINEAR / EXPONENTIAL CASES                    \n",
    "                    learning_rate_scheduler.step()\n",
    "                    print('----> LEARNING RATE = ', optimizer.param_groups[0]['lr'])\n",
    "                    \n",
    "                    if iteration == self.max_iterations:\n",
    "                        break\n",
    "\n",
    "                if iteration == self.max_iterations:\n",
    "                    break\n",
    "\n",
    "            if iteration == self.max_iterations:\n",
    "                break\n",
    "\n",
    "            print(\"Epoch {}/{} DONE!\".format(epoch, self.number_epochs))\n",
    "            \n",
    "        output_statistics_df = pd.DataFrame.from_dict(output_statistics, orient='index', \n",
    "                                                      columns=['iteration', 'mseloss', 'agedelta', 'lr'])     \n",
    "        output_statistics_df.to_csv(output_statistics_path)\n",
    "\n",
    "        self.LogWriter.close()\n",
    "\n",
    "        print('----------------------------------------')\n",
    "        print('NO TRAINING DONE TO PREVENT OVERFITTING!')\n",
    "        print('=====================')\n",
    "        end_time = datetime.now()\n",
    "        print('Completed At: {}'.format(end_time))\n",
    "        print('Training Duration: {}'.format(end_time - start_time))\n",
    "        print('****************************************************************')\n",
    "        \n",
    "train(data_parameters, training_parameters, network_parameters, misc_parameters, base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(data_parameters, training_parameters, network_parameters, misc_parameters, base_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
