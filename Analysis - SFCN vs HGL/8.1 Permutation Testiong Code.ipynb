{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "cd /well/win-fmrib-analysis/users/hsv459/multi-age-mapper/permutation\n",
    "\n",
    "module purge\n",
    "# module load Python/3.7.4-GCCcore-8.3.0\n",
    "module load Python/3.8.2-GCCcore-9.3.0\n",
    "\n",
    "CPU_ARCHITECTURE=$(/apps/misc/utils/bin/get-cpu-software-architecture.py)\n",
    "\n",
    "# Error handling\n",
    "if [[ ! $? == 0 ]]; then\n",
    "  echo \"Fatal error: Please send the following information to the BMRC team: Could not determine CPU software architecture on $(hostname)\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "# Activate the ivybridge or skylake version of your python virtual environment\n",
    "# source /well/win-fmrib-analysis/users/hsv459/python/functionmapper-${CPU_ARCHITECTURE}/bin/activate\n",
    "source /well/win-fmrib-analysis/users/hsv459/python/cpu_analysis-${CPU_ARCHITECTURE}/bin/activate  \n",
    "\n",
    "ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b75fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, permutation_test\n",
    "import sys\n",
    "import argparse\n",
    "import h5py\n",
    "from scipy.stats import t as student_t\n",
    "from statsmodels.stats import multitest as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconfound_inputs(y):\n",
    "    \n",
    "    y = y - y.mean(axis=0)\n",
    "    if np.sum(np.isnan(y)) == 0:\n",
    "        beta = np.linalg.pinv(confounds).dot(y)\n",
    "        beta[np.abs(beta) < 1e-10] = 0\n",
    "        yd = y - confounds.dot(beta)\n",
    "        yd = yd - yd.mean()\n",
    "    else:\n",
    "        print('ERROR! Encoundered a NaN! Function Needs Updating!')\n",
    "    \n",
    "    return yd\n",
    "\n",
    "def apply_FDR_correction(p_values):\n",
    "    '''\n",
    "    Code addapted from Emma Bluemke & Nicola Dinsdale\n",
    "    Wrapper for Benjamini/Hochberg (non-negative) p-value correction for multiple tests.\n",
    "    '''\n",
    "    p_values_corrected = mt.multipletests(p_values, alpha=0.05, method='fdr_bh')[1] \n",
    "    \n",
    "    return p_values_corrected\n",
    "\n",
    "\n",
    "def standardize_data(variables):\n",
    "    '''\n",
    "    Code addapted from Emma Bluemke & Nicola Dinsdale\n",
    "    '''\n",
    "\n",
    "    number_of_subjects=variables.shape[0]\n",
    "\n",
    "    # Compute the arithmetic mean & std along the specified axis, ignoring NaNs.\n",
    "    variables_mean_ignore_NaNs = np.nanmean(variables,axis=0)\n",
    "    variables_std_ignore_NaNs = np.nanstd(variables,axis=0)\n",
    "\n",
    "    # We standardize the data\n",
    "    variables_scaled = variables - np.tile(variables_mean_ignore_NaNs,(number_of_subjects,1))\n",
    "    variables_scaled = variables_scaled / np.tile(variables_std_ignore_NaNs,(number_of_subjects,1))\n",
    "\n",
    "    # Calculate how many variables are non NaN\n",
    "    number_of_non_NaN =np.sum(np.isnan(variables)==False,axis=0) #np.nanstd has N**0.5 in divisor\n",
    "\n",
    "    return variables_scaled, number_of_non_NaN\n",
    "\n",
    "\n",
    "def align_subjects(subjects_df, subjects_h5, age_delta_decon, subjects_to_be_ignored):\n",
    "    '''\n",
    "    Here we make sure that the subjects match between the results and the nIDP/IDP dataframes\n",
    "    The subjects_df uses slightly older data, and subjects might have left the study since it's generation\n",
    "    If need be, this will be corrected at a later stage!\n",
    "    '''\n",
    "    \n",
    "    if len(subjects_df) == len(subjects_h5):\n",
    "        assert (subjects_df == subjects_h5).all()\n",
    "    else:\n",
    "        idx_elim = np.where(subjects_df == subjects_to_be_ignored)[0][0]\n",
    "        subjects_df = np.delete(subjects_df, idx_elim)\n",
    "        assert (subjects_df == subjects_h5).all()\n",
    "        age_delta_decon = np.delete(age_delta_decon, idx_elim)\n",
    "    return subjects_df, age_delta_decon\n",
    "\n",
    "\n",
    "def statistic(x, y):\n",
    "    return pearsonr(x,y)[0]\n",
    "    \n",
    "\n",
    "def correlate_with_fdr_correction(deltas, ukb_variables, permutation_n_resamples=5000):\n",
    "    \"\"\"\n",
    "    Code addapted from Emma Bluemke & Nicola Dinsdale\n",
    "\n",
    "    correlate age deltas with rows in ukb_variables\n",
    "    returns pearson_r, t_test_statistic, p_values_corrected, p_values of length number_of_variables\n",
    "\n",
    "    pearson_r = raw Pearson correlation value (between -1 and 1)\n",
    "    t_test_statistic t test statistic for each variable ()\n",
    "    p_values_corrected is corrected p-values values (fdr corrected)\n",
    "    p_values is non corrected p-values\n",
    "    \"\"\"\n",
    "\n",
    "    ukb_variables_scaled , ukb_variales_number_of_non_NaN = standardize_data(ukb_variables)\n",
    "    ukb_variables_NaN_matrix = np.isnan(ukb_variables_scaled)\n",
    "    ukb_variables_scaled[ukb_variables_NaN_matrix] = 0\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    number_of_variables = ukb_variables_scaled.shape[1]\n",
    "\n",
    "    deltas_array = np.tile(deltas,(number_of_variables,1)).T\n",
    "    deltas_array[ukb_variables_NaN_matrix] = np.nan\n",
    "    deltas_array = standardize_data(deltas_array)[0]\n",
    "    deltas_array[ukb_variables_NaN_matrix] = 0\n",
    "\n",
    "#     pearson_r = np.sum(deltas_array * ukb_variables_scaled, axis=0) / ukb_variales_number_of_non_NaN\n",
    "    pearson_r = np.zeros(deltas_array.shape[1])\n",
    "    p_values = np.zeros(deltas_array.shape[1])\n",
    "    \n",
    "    for idx in range(pearson_r.shape[0]):\n",
    "        # CHANGE HERE THAT the zeros at nan-matrix locations are discarded, so we have only valid values for permutation testing\n",
    "        deltas_array_permute = np.delete(deltas_array[:,idx], ukb_variables_NaN_matrix[:,idx])\n",
    "        ukb_variables_scaled_permute = np.delete(ukb_variables_scaled[:,idx], ukb_variables_NaN_matrix[:,idx])\n",
    "        \n",
    "        res = permutation_test(\n",
    "            (deltas_array_permute, ukb_variables_scaled_permute), \n",
    "            statistic, \n",
    "            permutation_type='pairings', \n",
    "            n_resamples=permutation_n_resamples, \n",
    "            random_state=1\n",
    "        )\n",
    "        \n",
    "        pearson_r[idx] = res.statistic\n",
    "        p_values[idx] = res.pvalue\n",
    "        \n",
    "        print(\"\\r Processed {:.3f}%: {}/{} permutations\".format(idx/len(pearson_r) * 100.0, idx+1, len(pearson_r)), end='')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    t_test_statistic = pearson_r * ((ukb_variales_number_of_non_NaN-2)/(1-pearson_r**2))**0.5\n",
    "    t_test_statistic = np.abs(t_test_statistic)\n",
    "    t_test_statistic[ukb_variales_number_of_non_NaN <= 2] = np.nan #set those that had d.o.f 0 to nan\n",
    "\n",
    "#     p_values=np.zeros(len(t_test_statistic))\n",
    "    for j in range(len(t_test_statistic)):\n",
    "        if t_test_statistic[j] is not np.nan:\n",
    "#             p_values[j] = student_t.sf(t_test_statistic[j], ukb_variales_number_of_non_NaN[j] - 2) * 2\n",
    "            pass\n",
    "        else:\n",
    "            p_values[j]=np.nan\n",
    "\n",
    "    p_values_NaN_matrix = np.isnan(p_values)\n",
    "    p_values_no_NaN = p_values[p_values_NaN_matrix==False]\n",
    "    p_values_corrected_raw = apply_FDR_correction(p_values_no_NaN)\n",
    "\n",
    "    p_values_corrected=np.zeros(len(p_values))\n",
    "    p_values_corrected[p_values_NaN_matrix==False] = p_values_corrected_raw\n",
    "    p_values_corrected[p_values_NaN_matrix==True] = np.nan\n",
    "\n",
    "    return pearson_r, t_test_statistic, p_values_corrected, p_values\n",
    "\n",
    "\n",
    "def correlations_and_plots(age_delta_decon, deconf_subset, names_subset, idxs, categories_subset, title, \n",
    "                           corr_flag='IDP', permutation_n_resamples=5000):\n",
    "    \n",
    "    corr_results = correlate_with_fdr_correction(age_delta_decon, deconf_subset, \n",
    "                                                 permutation_n_resamples=permutation_n_resamples)\n",
    "    \n",
    "    corr_df = pd.DataFrame.from_dict({\n",
    "        'idx': idxs,\n",
    "        \"names\": names_subset,\n",
    "        'Categories': categories_subset,\n",
    "        \"pearson_r\": corr_results[0],\n",
    "        \"t_test_statistic\": corr_results[1],\n",
    "        \"p_values_corrected\": corr_results[2],\n",
    "        \"p_values\": corr_results[3],\n",
    "        \"abs_pearson_r\": np.abs(corr_results[0]),\n",
    "        \"log_p_values\": -np.log10(corr_results[3]),\n",
    "        \"log_p_values_corrected\": -np.log10(corr_results[2]),\n",
    "    })\n",
    "    \n",
    "    if corr_flag == 'IDP':\n",
    "        short_p_values = np.extract(corr_df.p_values !=0, corr_df.p_values)\n",
    "        bonferroni_threshold = 0.05/len(short_p_values) #... Bonferroni\n",
    "    else:\n",
    "        bonferroni_threshold = 0.05/len(corr_df.p_values) #... Bonferroni\n",
    "    bonferroni_threshold = -np.log10(bonferroni_threshold)\n",
    "\n",
    "    arguments = np.argsort(corr_df.p_values_corrected)\n",
    "    sorted_p_values_corrected = corr_df.p_values_corrected[arguments]\n",
    "    sorted_p_values = corr_df.p_values[arguments]\n",
    "    if len(sorted_p_values_corrected[sorted_p_values_corrected<=0.05]):\n",
    "        false_discovery_rate = np.nanmax(sorted_p_values[sorted_p_values_corrected<=0.05])\n",
    "        false_discovery_rate = -np.log10(false_discovery_rate)\n",
    "    else:\n",
    "\n",
    "        false_discovery_rate = None\n",
    "    \n",
    "    return corr_df, bonferroni_threshold, false_discovery_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_to_be_ignored = [21269692]\n",
    "subject_sex = 'female'\n",
    "prototype_flag = True\n",
    "permutation_n_resamples = 276200 # otherwise 5000\n",
    "load_top_flag = True\n",
    "\n",
    "deconfound_flag = True\n",
    "vars_cutoff_nan = 0.1\n",
    "\n",
    "# Identify Correct File to Load\n",
    "\n",
    "IDP_file = 'female_test_IDPs.h5'\n",
    "confounds_file = 'female_test_conf12.h5'\n",
    "data_file = 'female_test.pkl'\n",
    "\n",
    "confounds = h5py.File(confounds_file, 'r')\n",
    "confounds = confounds['conf12'][()]\n",
    "\n",
    "IDP_h5 = h5py.File(IDP_file, 'r')\n",
    "subjects_h5 = IDP_h5['subjects'][()]\n",
    "\n",
    "# Load the relevant dataframe based on the required modality\n",
    "\n",
    "df = pd.read_pickle(data_file)\n",
    "\n",
    "IDP_h5 = h5py.File(IDP_file, 'r')\n",
    "vars_i_deconf = IDP_h5['vars_i_deconf'][()]\n",
    "subjects_h5 = IDP_h5['subjects'][()]\n",
    "\n",
    "with open('varsHeader.txt') as f:\n",
    "    varsHeader = f.readlines()\n",
    "    varsHeader = [l.strip('\\n\\r') for l in varsHeader]\n",
    "    varsHeader = np.array(varsHeader)\n",
    "vars_categories = np.loadtxt(\"vars_categories.txt\", dtype=str, delimiter='\\n')\n",
    "\n",
    "varsIDX = np.arange(0, len(varsHeader), 1).astype(int)\n",
    "\n",
    "vars_perc_not_nan = np.sum(np.isnan(vars_i_deconf)==False,axis=0) / vars_i_deconf.shape[0]\n",
    "vars_idx_nan_cutoff = np.where(vars_perc_not_nan >= vars_cutoff_nan)[0]\n",
    "\n",
    "varsHeader_subset = varsHeader[vars_idx_nan_cutoff]\n",
    "vars_i_deconf_subset = vars_i_deconf[:, vars_idx_nan_cutoff]\n",
    "vars_categories_subset = vars_categories[vars_idx_nan_cutoff]\n",
    "varsIDX_subset = varsIDX[vars_idx_nan_cutoff]\n",
    "    \n",
    "del IDP_h5\n",
    "\n",
    "vars_corrs = []\n",
    "vars_mods = []\n",
    "vars_bnf_thrs = []\n",
    "vars_fdr_thrs = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    \n",
    "    print('Solving Dataframe: ', idx+1)\n",
    "    \n",
    "    if load_top_flag == True:\n",
    "        \n",
    "        filename = 'logpOver2_' + str(idx) + '.npz'\n",
    "        print(filename)\n",
    "        \n",
    "        npzfile = np.load(filename)\n",
    "        varsHeader_subset = np.copy(npzfile['arr_0'])\n",
    "        vars_i_deconf_subset = np.copy(npzfile['arr_1']).T\n",
    "        vars_categories_subset = np.copy(npzfile['arr_2'])\n",
    "        varsIDX_subset = np.copy(npzfile['arr_3'])\n",
    "        del npzfile\n",
    "    \n",
    "    df_mod = df.iloc[idx].dataframe\n",
    "    modality = df.iloc[idx].modality\n",
    "    age_delta_decon = df_mod.age_delta_decon.to_numpy()\n",
    "    subjects = df_mod['Unnamed: 0'].to_numpy()\n",
    "    subjects, age_delta_decon = align_subjects(subjects, subjects_h5, \n",
    "                                                  age_delta_decon, subjects_to_be_ignored)\n",
    "    \n",
    "    if deconfound_flag == True:\n",
    "        age_delta_decon = deconfound_inputs(age_delta_decon)\n",
    "        \n",
    "        \n",
    "    corr_df, bonferroni_threshold, false_discovery_rate = correlations_and_plots(\n",
    "                                    age_delta_decon = age_delta_decon, deconf_subset = vars_i_deconf_subset, \n",
    "                                    names_subset = varsHeader_subset, idxs = varsIDX_subset,\n",
    "                                    categories_subset = vars_categories_subset,\n",
    "                                    title = modality, corr_flag='vars', \n",
    "                                    permutation_n_resamples=permutation_n_resamples)\n",
    "    vars_mods.append(modality)\n",
    "    vars_corrs.append(corr_df)\n",
    "    vars_bnf_thrs.append(bonferroni_threshold)\n",
    "    vars_fdr_thrs.append(false_discovery_rate)\n",
    "    \n",
    "    if idx==3:\n",
    "        break\n",
    "    \n",
    "vars_df = pd.DataFrame.from_dict({\n",
    "    'modality': vars_mods,\n",
    "    'bonf': vars_bnf_thrs,\n",
    "    'fdr': vars_fdr_thrs,\n",
    "    'dataframe': vars_corrs\n",
    "})\n",
    "\n",
    "vars_df.to_pickle('female_vars_permutation_tested_high.pkl', protocol = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
