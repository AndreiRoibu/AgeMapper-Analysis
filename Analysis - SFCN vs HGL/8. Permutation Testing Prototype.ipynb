{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b7840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, permutation_test\n",
    "import sys\n",
    "import argparse\n",
    "import h5py\n",
    "from scipy.stats import t as student_t\n",
    "from statsmodels.stats import multitest as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c5db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconfound_inputs(y):\n",
    "    \n",
    "    y = y - y.mean(axis=0)\n",
    "    if np.sum(np.isnan(y)) == 0:\n",
    "        beta = np.linalg.pinv(confounds).dot(y)\n",
    "        beta[np.abs(beta) < 1e-10] = 0\n",
    "        yd = y - confounds.dot(beta)\n",
    "        yd = yd - yd.mean()\n",
    "    else:\n",
    "        print('ERROR! Encoundered a NaN! Function Needs Updating!')\n",
    "    \n",
    "    return yd\n",
    "\n",
    "def apply_FDR_correction(p_values):\n",
    "    '''\n",
    "    Code addapted from Emma Bluemke & Nicola Dinsdale\n",
    "    Wrapper for Benjamini/Hochberg (non-negative) p-value correction for multiple tests.\n",
    "    '''\n",
    "    p_values_corrected = mt.multipletests(p_values, alpha=0.05, method='fdr_bh')[1] \n",
    "    \n",
    "    return p_values_corrected\n",
    "\n",
    "\n",
    "def standardize_data(variables):\n",
    "    '''\n",
    "    Code addapted from Emma Bluemke & Nicola Dinsdale\n",
    "    '''\n",
    "\n",
    "    number_of_subjects=variables.shape[0]\n",
    "\n",
    "    # Compute the arithmetic mean & std along the specified axis, ignoring NaNs.\n",
    "    variables_mean_ignore_NaNs = np.nanmean(variables,axis=0)\n",
    "    variables_std_ignore_NaNs = np.nanstd(variables,axis=0)\n",
    "\n",
    "    # We standardize the data\n",
    "    variables_scaled = variables - np.tile(variables_mean_ignore_NaNs,(number_of_subjects,1))\n",
    "    variables_scaled = variables_scaled / np.tile(variables_std_ignore_NaNs,(number_of_subjects,1))\n",
    "\n",
    "    # Calculate how many variables are non NaN\n",
    "    number_of_non_NaN =np.sum(np.isnan(variables)==False,axis=0) #np.nanstd has N**0.5 in divisor\n",
    "\n",
    "    return variables_scaled, number_of_non_NaN\n",
    "\n",
    "\n",
    "def statistic(x, y):\n",
    "    return pearsonr(x,y)[0]\n",
    "    \n",
    "\n",
    "def correlate_with_fdr_correction(deltas, ukb_variables):\n",
    "    \"\"\"\n",
    "    Code addapted from Emma Bluemke & Nicola Dinsdale\n",
    "\n",
    "    correlate age deltas with rows in ukb_variables\n",
    "    returns pearson_r, t_test_statistic, p_values_corrected, p_values of length number_of_variables\n",
    "\n",
    "    pearson_r = raw Pearson correlation value (between -1 and 1)\n",
    "    t_test_statistic t test statistic for each variable ()\n",
    "    p_values_corrected is corrected p-values values (fdr corrected)\n",
    "    p_values is non corrected p-values\n",
    "    \"\"\"\n",
    "\n",
    "    ukb_variables_scaled , ukb_variales_number_of_non_NaN = standardize_data(ukb_variables)\n",
    "    ukb_variables_NaN_matrix = np.isnan(ukb_variables_scaled)\n",
    "    ukb_variables_scaled[ukb_variables_NaN_matrix] = 0\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    number_of_variables = ukb_variables_scaled.shape[1]\n",
    "\n",
    "    deltas_array = np.tile(deltas,(number_of_variables,1)).T\n",
    "    deltas_array[ukb_variables_NaN_matrix] = np.nan\n",
    "    deltas_array = standardize_data(deltas_array)[0]\n",
    "    deltas_array[ukb_variables_NaN_matrix] = 0\n",
    "\n",
    "#     pearson_r = np.sum(deltas_array * ukb_variables_scaled, axis=0) / ukb_variales_number_of_non_NaN\n",
    "    pearson_r = np.zeros(deltas_array.shape[1])\n",
    "    p_values = np.zeros(deltas_array.shape[1])\n",
    "    \n",
    "    for idx in range(pearson_r.shape[0]):\n",
    "        res = permutation_test(\n",
    "            (deltas_array[:,idx], ukb_variables_scaled[:,idx]), \n",
    "            statistic, \n",
    "            permutation_type='pairings', \n",
    "            n_resamples=5000, \n",
    "            random_state=1\n",
    "        )\n",
    "        \n",
    "        pearson_r[idx] = res.statistic\n",
    "        p_values[idx] = res.pvalue\n",
    "    \n",
    "    \n",
    "    t_test_statistic = pearson_r * ((ukb_variales_number_of_non_NaN-2)/(1-pearson_r**2))**0.5\n",
    "    t_test_statistic = np.abs(t_test_statistic)\n",
    "    t_test_statistic[ukb_variales_number_of_non_NaN <= 2] = np.nan #set those that had d.o.f 0 to nan\n",
    "\n",
    "#     p_values=np.zeros(len(t_test_statistic))\n",
    "    for j in range(len(t_test_statistic)):\n",
    "        if t_test_statistic[j] is not np.nan:\n",
    "#             p_values[j] = student_t.sf(t_test_statistic[j], ukb_variales_number_of_non_NaN[j] - 2) * 2\n",
    "            pass\n",
    "        else:\n",
    "            p_values[j]=np.nan\n",
    "\n",
    "    p_values_NaN_matrix = np.isnan(p_values)\n",
    "    p_values_no_NaN = p_values[p_values_NaN_matrix==False]\n",
    "    p_values_corrected_raw = apply_FDR_correction(p_values_no_NaN)\n",
    "\n",
    "    p_values_corrected=np.zeros(len(p_values))\n",
    "    p_values_corrected[p_values_NaN_matrix==False] = p_values_corrected_raw\n",
    "    p_values_corrected[p_values_NaN_matrix==True] = np.nan\n",
    "\n",
    "    return pearson_r, t_test_statistic, p_values_corrected, p_values\n",
    "\n",
    "def align_subjects(subjects_df, subjects_h5, age_delta_decon, subjects_to_be_ignored):\n",
    "    '''\n",
    "    Here we make sure that the subjects match between the results and the nIDP/IDP dataframes\n",
    "    The subjects_df uses slightly older data, and subjects might have left the study since it's generation\n",
    "    If need be, this will be corrected at a later stage!\n",
    "    '''\n",
    "    \n",
    "    if len(subjects_df) == len(subjects_h5):\n",
    "        assert (subjects_df == subjects_h5).all()\n",
    "    else:\n",
    "        idx_elim = np.where(subjects_df == subjects_to_be_ignored)[0][0]\n",
    "        subjects_df = np.delete(subjects_df, idx_elim)\n",
    "        assert (subjects_df == subjects_h5).all()\n",
    "        age_delta_decon = np.delete(age_delta_decon, idx_elim)\n",
    "    return subjects_df, age_delta_decon\n",
    "\n",
    "def correlations_and_plots(age_delta_decon, deconf_subset, names_subset, idxs, categories_subset, title, \n",
    "                           corr_flag='IDP'):\n",
    "    \n",
    "    corr_results = correlate_with_fdr_correction(age_delta_decon, deconf_subset)\n",
    "    \n",
    "    corr_df = pd.DataFrame.from_dict({\n",
    "        'idx': idxs,\n",
    "        \"names\": names_subset,\n",
    "        'Categories': categories_subset,\n",
    "        \"pearson_r\": corr_results[0],\n",
    "        \"t_test_statistic\": corr_results[1],\n",
    "        \"p_values_corrected\": corr_results[2],\n",
    "        \"p_values\": corr_results[3],\n",
    "        \"abs_pearson_r\": np.abs(corr_results[0]),\n",
    "        \"log_p_values\": -np.log10(corr_results[3]),\n",
    "        \"log_p_values_corrected\": -np.log10(corr_results[2]),\n",
    "    })\n",
    "    \n",
    "    if corr_flag == 'IDP':\n",
    "        short_p_values = np.extract(corr_df.p_values !=0, corr_df.p_values)\n",
    "        bonferroni_threshold = 0.05/len(short_p_values) #... Bonferroni\n",
    "    else:\n",
    "        bonferroni_threshold = 0.05/len(corr_df.p_values) #... Bonferroni\n",
    "    bonferroni_threshold = -np.log10(bonferroni_threshold)\n",
    "\n",
    "    arguments = np.argsort(corr_df.p_values_corrected)\n",
    "    sorted_p_values_corrected = corr_df.p_values_corrected[arguments]\n",
    "    sorted_p_values = corr_df.p_values[arguments]\n",
    "    if len(sorted_p_values_corrected[sorted_p_values_corrected<=0.05]):\n",
    "        false_discovery_rate = np.nanmax(sorted_p_values[sorted_p_values_corrected<=0.05])\n",
    "        false_discovery_rate = -np.log10(false_discovery_rate)\n",
    "    else:\n",
    "\n",
    "        false_discovery_rate = None\n",
    "    \n",
    "    return corr_df, bonferroni_threshold, false_discovery_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb972728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3932510402137002, 0.44053086470329966)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "rng = np.random.default_rng()\n",
    "x = norm.rvs(size=6, random_state=rng)\n",
    "y = norm.rvs(size=6, loc = 3, random_state=rng)\n",
    "statistic = stats.pearsonr\n",
    "statistic(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddf7704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = norm.rvs(size=6, random_state=rng)\n",
    "y = norm.rvs(size=(2,6), loc = 3, random_state=rng)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650ef0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3966e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.permutation_test((x, y), statistic,\n",
    "                        n_resamples=np.inf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71d5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09598688  0.85646187]\n"
     ]
    }
   ],
   "source": [
    "print(res.statistic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4c5b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01298701 0.11688312]\n"
     ]
    }
   ],
   "source": [
    "print(res.pvalue[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc77f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06086362  0.9088173 ]\n"
     ]
    }
   ],
   "source": [
    "res = stats.permutation_test((x, y[1]), statistic,\n",
    "                        n_resamples=np.inf)\n",
    "print(res.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb22a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01182135520077738\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m t0\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermutation_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatistic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermutation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpairings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m t1\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(t1 \u001b[38;5;241m-\u001b[39m t0, (t1\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/agemapper2/lib/python3.10/site-packages/scipy/stats/_hypotests.py:2089\u001b[0m, in \u001b[0;36mpermutation_test\u001b[0;34m(data, statistic, permutation_type, vectorized, n_resamples, batch, alternative, axis, random_state)\u001b[0m\n\u001b[1;32m   2085\u001b[0m null_calculator_args \u001b[38;5;241m=\u001b[39m (data, statistic, n_resamples,\n\u001b[1;32m   2086\u001b[0m                         batch, random_state)\n\u001b[1;32m   2087\u001b[0m calculate_null \u001b[38;5;241m=\u001b[39m null_calculators[permutation_type]\n\u001b[1;32m   2088\u001b[0m null_distribution, n_resamples, exact_test \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2089\u001b[0m     \u001b[43mcalculate_null\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnull_calculator_args\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# See References [2] and [3]\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m adjustment \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exact_test \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/agemapper2/lib/python3.10/site-packages/scipy/stats/_hypotests.py:1609\u001b[0m, in \u001b[0;36m_calculate_null_pairings\u001b[0;34m(data, statistic, n_permutations, batch, random_state)\u001b[0m\n\u001b[1;32m   1606\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(n_permutations)\n\u001b[1;32m   1607\u001b[0m null_distribution \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1609\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m _batch_generator(perm_generator, batch\u001b[38;5;241m=\u001b[39mbatch):\n\u001b[1;32m   1610\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(indices)\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;66;03m# `indices` is 3D: the zeroth axis is for permutations, the next is\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m     \u001b[38;5;66;03m# for samples, and the last is for observations. Swap the first two\u001b[39;00m\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;66;03m# to make the zeroth axis correspond with samples, as it does for\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;66;03m# `data`.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/agemapper2/lib/python3.10/site-packages/scipy/stats/_hypotests.py:1509\u001b[0m, in \u001b[0;36m_batch_generator\u001b[0;34m(iterable, batch)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;66;03m# get elements from iterator `batch` at a time\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch):\n\u001b[0;32m-> 1509\u001b[0m         z\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m z\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# when there are no more elements, yield the final batch and stop\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/agemapper2/lib/python3.10/site-packages/scipy/stats/_hypotests.py:1602\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     exact_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# Separate random permutations of indices for each sample.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# Again, it would be nice if RandomState/Generator.permutation\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# could permute each axis-slice separately.\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     perm_generator \u001b[38;5;241m=\u001b[39m ([random_state\u001b[38;5;241m.\u001b[39mpermutation(n_obs_sample)\n\u001b[1;32m   1603\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples)]\n\u001b[1;32m   1604\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_permutations))\n\u001b[1;32m   1606\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(n_permutations)\n\u001b[1;32m   1607\u001b[0m null_distribution \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/agemapper2/lib/python3.10/site-packages/scipy/stats/_hypotests.py:1602\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     exact_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# Separate random permutations of indices for each sample.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# Again, it would be nice if RandomState/Generator.permutation\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# could permute each axis-slice separately.\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     perm_generator \u001b[38;5;241m=\u001b[39m ([\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermutation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_obs_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1603\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples)]\n\u001b[1;32m   1604\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_permutations))\n\u001b[1;32m   1606\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(n_permutations)\n\u001b[1;32m   1607\u001b[0m null_distribution \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pearsonr = stats.pearsonr\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def statistic(x, y):\n",
    "    return pearsonr(x,y)[0]\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "rng = np.random.default_rng()\n",
    "# statistic = stats.pearsonr\n",
    "\n",
    "x = norm.rvs(size=5000, random_state=rng)\n",
    "y = norm.rvs(size=5000, loc=0.3, random_state=rng)\n",
    "print(statistic(x, y))\n",
    "\n",
    "t0=datetime.now()\n",
    "for i in range(100):\n",
    "    res = stats.permutation_test((x, y), statistic, permutation_type='pairings', n_resamples=5000, random_state=1)\n",
    "t1=datetime.now()\n",
    "print(t1 - t0, (t1-t0)/100)\n",
    "print(res.statistic)\n",
    "print(res.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cf8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_to_be_ignored = [21269692]\n",
    "subject_sex = 'female'\n",
    "prototype_flag = True\n",
    "\n",
    "deconfound_flag = True\n",
    "vars_cutoff_nan = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9e3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Correct File to Load\n",
    "\n",
    "IDP_file = '../Analsysis Female New/female_test_IDPs.h5'\n",
    "confounds_file = '../Analsysis Female New/female_test_conf12.h5'\n",
    "data_file = 'female_test.pkl'\n",
    "\n",
    "confounds = h5py.File(confounds_file, 'r')\n",
    "confounds = confounds['conf12'][()]\n",
    "\n",
    "IDP_h5 = h5py.File(IDP_file, 'r')\n",
    "subjects_h5 = IDP_h5['subjects'][()]\n",
    "\n",
    "# Load the relevant dataframe based on the required modality\n",
    "\n",
    "df = pd.read_pickle(data_file)\n",
    "\n",
    "IDP_h5 = h5py.File(IDP_file, 'r')\n",
    "vars_i_deconf = IDP_h5['vars_i_deconf'][()]\n",
    "subjects_h5 = IDP_h5['subjects'][()]\n",
    "\n",
    "with open('../Analsysis Female New/varsHeader.txt') as f:\n",
    "    varsHeader = f.readlines()\n",
    "    varsHeader = [l.strip('\\n\\r') for l in varsHeader]\n",
    "    varsHeader = np.array(varsHeader)\n",
    "with open('../Analsysis Female New/vars_categories.txt') as f:\n",
    "    vars_categories = f.readlines()\n",
    "    vars_categories = [l.strip('\\n\\r') for l in vars_categories]\n",
    "    vars_categories = np.array(vars_categories)\n",
    "\n",
    "varsIDX = np.arange(0, len(varsHeader), 1).astype(int)\n",
    "\n",
    "vars_perc_not_nan = np.sum(np.isnan(vars_i_deconf)==False,axis=0) / vars_i_deconf.shape[0]\n",
    "vars_idx_nan_cutoff = np.where(vars_perc_not_nan >= vars_cutoff_nan)[0]\n",
    "\n",
    "varsHeader_subset = varsHeader[vars_idx_nan_cutoff]\n",
    "vars_i_deconf_subset = vars_i_deconf[:, vars_idx_nan_cutoff]\n",
    "vars_categories_subset = vars_categories[vars_idx_nan_cutoff]\n",
    "varsIDX_subset = varsIDX[vars_idx_nan_cutoff]\n",
    "\n",
    "del IDP_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0660112",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vars_corrs = []\n",
    "vars_mods = []\n",
    "vars_bnf_thrs = []\n",
    "vars_fdr_thrs = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    df_mod = df.iloc[idx].dataframe\n",
    "    modality = df.iloc[idx].modality\n",
    "    age_delta_decon = df_mod.age_delta_decon.to_numpy()\n",
    "    subjects = df_mod['Unnamed: 0'].to_numpy()\n",
    "    subjects, age_delta_decon = align_subjects(subjects, subjects_h5, \n",
    "                                                  age_delta_decon, subjects_to_be_ignored)\n",
    "    \n",
    "    if deconfound_flag == True:\n",
    "        age_delta_decon = deconfound_inputs(age_delta_decon)\n",
    "        \n",
    "        \n",
    "    corr_df, bonferroni_threshold, false_discovery_rate = correlations_and_plots(\n",
    "                                    age_delta_decon = age_delta_decon, deconf_subset = vars_i_deconf_subset, \n",
    "                                    names_subset = varsHeader_subset, idxs = varsIDX_subset,\n",
    "                                    categories_subset = vars_categories_subset,\n",
    "                                    title = modality, corr_flag='vars')\n",
    "    vars_mods.append(modality)\n",
    "    vars_corrs.append(corr_df)\n",
    "    vars_bnf_thrs.append(bonferroni_threshold)\n",
    "    vars_fdr_thrs.append(false_discovery_rate)\n",
    "    \n",
    "    if idx==3:\n",
    "        break\n",
    "    \n",
    "vars_df = pd.DataFrame.from_dict({\n",
    "    'modality': vars_mods,\n",
    "    'bonf': vars_bnf_thrs,\n",
    "    'fdr': vars_fdr_thrs,\n",
    "    'dataframe': vars_corrs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1324e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_df.to_pickle('female_vars_permutation_tested.pkl', protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487de64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "df_mod = df.iloc[idx].dataframe\n",
    "modality = df.iloc[idx].modality\n",
    "age_delta_decon = df_mod.age_delta_decon.to_numpy()\n",
    "subjects = df_mod['Unnamed: 0'].to_numpy()\n",
    "subjects, age_delta_decon = align_subjects(subjects, subjects_h5, \n",
    "                                              age_delta_decon, subjects_to_be_ignored)\n",
    "\n",
    "if deconfound_flag == True:\n",
    "    age_delta_decon = deconfound_inputs(age_delta_decon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866b12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = np.copy(age_delta_decon)\n",
    "ukb_variables = vars_i_deconf_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737e5dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pearson_r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mpearson_r\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pearson_r' is not defined"
     ]
    }
   ],
   "source": [
    "type(pearson_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "102ae072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5021, 13809)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ukb_variables.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bd94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_variables_scaled , ukb_variales_number_of_non_NaN = standardize_data(ukb_variables)\n",
    "ukb_variables_NaN_matrix = np.isnan(ukb_variables_scaled)\n",
    "ukb_variables_scaled[ukb_variables_NaN_matrix] = 0\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "number_of_variables = ukb_variables_scaled.shape[1]\n",
    "\n",
    "deltas_array = np.tile(deltas,(number_of_variables,1)).T\n",
    "deltas_array[ukb_variables_NaN_matrix] = np.nan\n",
    "deltas_array = standardize_data(deltas_array)[0]\n",
    "deltas_array[ukb_variables_NaN_matrix] = 0\n",
    "\n",
    "pearson_r = np.sum(deltas_array * ukb_variables_scaled, axis=0) / ukb_variales_number_of_non_NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d3b548f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05016419856295517"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(deltas_array[:,2], ukb_variables_scaled[:,2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6efa1012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19415069e-02, -9.71047499e-05, -5.01641986e-02])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_r[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7b8126ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13809,)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f2339ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13809"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas_array.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3d1b7b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13809,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty(deltas_array.shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cb5d8cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5021,)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas_array[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2c02f774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02383519         nan         nan ...         nan  0.02383519\n",
      "   0.02383519]\n",
      " [-0.26473594         nan         nan ...         nan -0.26473594\n",
      "  -0.26473594]\n",
      " [ 0.67400042         nan         nan ...         nan  0.67400042\n",
      "   0.67400042]\n",
      " ...\n",
      " [-2.27160436         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [ 2.55569929         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [ 0.913351           nan  0.913351   ...         nan  0.913351\n",
      "   0.913351  ]]\n",
      "------\n",
      "[-0.00034379 -0.11382293 -0.02613652 ... -0.07509479 -0.03095923\n",
      " -0.03095923]\n"
     ]
    }
   ],
   "source": [
    "deltas_array = np.tile(deltas,(number_of_variables,1)).T\n",
    "deltas_array[ukb_variables_NaN_matrix] = np.nan\n",
    "\n",
    "variables=deltas_array\n",
    "\n",
    "print(variables)\n",
    "print('------')\n",
    "\n",
    "number_of_subjects=variables.shape[0]\n",
    "\n",
    "# Compute the arithmetic mean & std along the specified axis, ignoring NaNs.\n",
    "variables_mean_ignore_NaNs = np.nanmean(variables,axis=0)\n",
    "variables_std_ignore_NaNs = np.nanstd(variables,axis=0)\n",
    "\n",
    "print(variables_mean_ignore_NaNs)\n",
    "\n",
    "# We standardize the data\n",
    "variables_scaled = variables - np.tile(variables_mean_ignore_NaNs,(number_of_subjects,1))\n",
    "variables_scaled = variables_scaled / np.tile(variables_std_ignore_NaNs,(number_of_subjects,1))\n",
    "\n",
    "# Calculate how many variables are non NaN\n",
    "number_of_non_NaN =np.sum(np.isnan(variables)==False,axis=0) #np.nanstd has N**0.5 in divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1496d79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13809"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pearson_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "64136c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.21.6'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db4a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
